# -*- coding: utf-8 -*-
"""Sprint_15-Challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oaFKZDyKjWuC64Rt4YecrfV7A3-0pD7Q

Large Language Model Challenge: LLM StoryBot
Data Science Sprint 15 Challenge
Challenge Objectives üßæ
You should be able to...

Implement an LLM (either local or API based)
Customize the behavior of an LLM
Use a custom LLM in your terminal
You may use any Python libraries that can be installed via pip, including the Python standard library. Please do not use any websites like ChatGPT to write your stories. You are welcome to use them to help write the code for your custom model, though! Pro Tip: ask them to act as teachers to help you learn rather than just getting them to write the code for you.

Part 1 - Choose an LLM Model üîç
The easy choice here is OpenAi's API version of GPT, but it requires a paid account. If you want a higher-level challenge, choose an open-source model that runs locally. Although GPT is easier to set up and performs better than open-source models, you will learn way more if you implement a local open-source model.

Part 2 - Implement an LLM Model as a Brilliant Storyteller Bot ü§ñ
Design an LLM that acts like a brilliant storyteller by having it generate a story based on user input. The stories should be about 1000 words in length or longer.

Be mindful of the context window of the model you choose. The system_prompt, user_prompt, and output combined need to fit inside one context window. See the documentation for the model you're using for more details. Pro Tip: choose a model with good documentation! üòè
"""

#from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")

class StoryBot:
    def __init__(self):

        #self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        self.tokenizer = tokenizer
        self.model = AutoModelForCausalLM.from_pretrained("gpt2")
        #self.model = GPT2LMHeadModel.from_pretrained("gpt2")

    def __call__(self, user_prompt: str, max_length: int = 1000) -> str:
        input_ids = self.tokenizer.encode(user_prompt, return_tensors="pt")
        output = self.model.generate(input_ids, max_length=max_length, num_return_sequences=1)
        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return generated_text

"""Part 3 - StoryBot Testing üß™
Have your bot write a few stories and evaluate them. Can alterations be made to the model's parameters to improve the results?
"""

tokenizer_pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("gpt2")
model.config.pad_token_id = model.config.eos_token_id

user_prompt = "A short film about a pilot who "
story_bot = StoryBot()
story_bot(user_prompt)

"""Part 4 - Story Submission üìú
Have your bot generate at least three stories and choose your favorite to upload for evaluation. The story should be 1000 words or more and be purely AI-generated. Please do not edit the story in any way. To submit your Sprint Challenge, upload your story.txt file by itself.

Stretch Goals (Optional) üèÑ
Have the call method of the bot output a story as a txt file rather than printing to the terminal, so you don't need to copy/paste for submission.
Give your bot a distinct personality on top of its ability to write captivating stories.
Implement a memory module for your bot. A memory module would give you the ability to have it do multiple revisions of the same story based on your input.
Implement an API (Flask or FastAPI) to interact with your bot.

Congratulations! ü•≥
Thank you for your hard work, and congratulations!!! You've learned a lot, and you should proudly call yourself a Data Scientist.
"""